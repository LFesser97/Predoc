{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "iJozf_gxBXEC",
        "outputId": "14b0e653-fa05-4157-e391-5ed5983fede4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndal.ipynb\\n\\nCreated on Mar 30 2023\\n\\n@author: Lukas\\n\\nThis notebook is meant as an introduction to discriminative active learning (DAL),\\nand contains implementations for computing the H-Divergence between datasets and \\nfor running DAL.\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "h-divergence.ipynb\n",
        "\n",
        "Created on Mar 30 2023\n",
        "\n",
        "@author: Lukas\n",
        "\n",
        "This notebook is meant as an introduction to discriminative active learning (DAL),\n",
        "and contains implementations for computing the H-Divergence between datasets.\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CfY-pyZqBXEJ"
      },
      "source": [
        "First, we recall the definition of the H-Divergence. See here (https://melissadell.atlassian.net/wiki/spaces/TCC/pages/2584412161) for more background information and links to domain adaptation.\n",
        "\n",
        "*Definition (H-Divergence):* Let $X$ be a domain (dataset), and let $D_S$ and $D_T$ be two distributions over $X$ (source and target). Let $H$ be a hypothesis class over $X$ (set of possible classifiers). Then we define the $H$-Divergence between $D_S$ and $D_T$ as\n",
        "\n",
        "$d_H(D_S, D_T) = \\sup_{h \\in H} \\left| \\mathbb{P}_{x \\sim D_S} \\left[ h(x) = 1 \\right] - \\mathbb{P}_{x \\sim D_T} \\left[ h(x) = 1 \\right]\\right|$\n",
        "\n",
        "We are interested in the distributions of the labeled and unlabed datasets, denoted by $L$ and $U$, respectively, so the $H$-Divergence becomes\n",
        "\n",
        "$d_H(D_S, D_T) = \\sup_{h \\in H} \\left| \\frac{1}{|L|} \\sum_{x \\in L} h(x) - \\frac{1}{|U|} \\sum_{x \\in U }h(x) \\right|$\n",
        "\n",
        "where $h(x)$ denotes the probability which the model $h$ assigns to the event $x \\in L$. In order to (approximately) attain the supremum, we train a binary MLP classifier with the above expression as the loss function, i.e. we want it to output very high $h(x)$ for $x \\in L$ and very low $h(x)$ for $x \\in U$. We follow the original DAL paper and apply the classifier $h$ to the embeddings $\\phi(x)$, and not to the original data $x$ itself. Here, $\\phi$ is the model we would ultimately like to train using active learning (e.g. BERT).\n",
        "\n",
        "Note that, by definition, $d_H(D_S, D_T) \\in [0, 1]$, where we want $d_H(D_S, D_T) \\approx 0$, which would indicate that the model (on average) cannot distinguish between $L$ and $U$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "emmWESI3BXEM"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "\n",
        "import numpy as np\n",
        "import torch, torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1whF8KoBXEN"
      },
      "source": [
        "**Basic functionality for computing the H-Divergence between two datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tIFEcwz_BXEO"
      },
      "outputs": [],
      "source": [
        "# compute the H-divergence between the labeled data and the unlabeled data\n",
        "\n",
        "def compute_H_divergence(labeled, unlabeled, model):\n",
        "    \"\"\"\n",
        "    A function that computes the H-divergence between the labeled and unlabeled data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labeled : numpy array\n",
        "        The labeled data.\n",
        "\n",
        "    unlabeled : numpy array\n",
        "        The unlabeled data.\n",
        "\n",
        "    model : tensorflow model\n",
        "        The discriminative model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    H_divergence : float\n",
        "        The H-divergence between the labeled and unlabeled data.\n",
        "        Must be between 0 and 1.\n",
        "    \"\"\"\n",
        "    # for each element in the labeled data, compute the probabilities of the classes\n",
        "    p_L = model.predict(labeled)\n",
        "\n",
        "    # for each element in the unlabeled data, compute the probabilities of the classes\n",
        "    p_U = model.predict(unlabeled)\n",
        "\n",
        "    # sum the probabilities of class 0 for each element in the labeled data and divide by the number of elements\n",
        "    p_L_0 = np.sum(p_L[:, 0])\n",
        "    p_L_0 /= labeled.shape[0]\n",
        "\n",
        "    # sum the probabilities of class 0 for each element in the unlabeled data and divide by the number of elements\n",
        "    p_U_0 = np.sum(p_U[:, 0])\n",
        "    p_U_0 /= unlabeled.shape[0]\n",
        "\n",
        "    # compute the H-divergence as the absolute difference between p_U_0 and p_L_0\n",
        "    H_divergence = np.abs(p_U_0 - p_L_0)\n",
        "\n",
        "    return H_divergence\n",
        "\n",
        "\n",
        "# train a discriminative model on the labeled data and unlabeled data\n",
        "\n",
        "def train_discriminative_model(labeled, unlabeled, input_shape):\n",
        "    \"\"\"\n",
        "    A function that trains and returns a discriminative model on the labeled and unlabeled data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labeled : numpy.ndarray\n",
        "        The labeled data.\n",
        "\n",
        "    unlabeled : numpy.ndarray\n",
        "        The unlabeled data.\n",
        "\n",
        "    input_shape : int\n",
        "        The number of features in the dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : tf.keras.Sequential\n",
        "        The trained discriminative model.\n",
        "    \"\"\"\n",
        "\n",
        "    # create the binary dataset:\n",
        "    y_L = np.zeros((labeled.shape[0], 1), dtype='int')\n",
        "    y_U = np.ones((unlabeled.shape[0], 1), dtype='int')\n",
        "    X_train = np.vstack((labeled, unlabeled))\n",
        "    Y_train = np.vstack((y_L, y_U))\n",
        "    X_train = torch.from_numpy(X_train).float()\n",
        "\n",
        "    # build the model:\n",
        "    model = get_discriminative_model(input_shape)\n",
        "\n",
        "    # train the model using torch:\n",
        "    batch_size = 100\n",
        "    epochs = 10\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, X_train.shape[0], batch_size):\n",
        "            x = X_train[i:i + batch_size]\n",
        "            y = Y_train[i:i + batch_size]\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# we use a 3-layer MLP as the discriminative model\n",
        "\n",
        "def get_discriminative_model(input_shape):\n",
        "    \"\"\"\n",
        "    The MLP model for discriminative active learning, without any regularization techniques.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_shape : int\n",
        "        The number of features in the dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : tf.keras.Sequential\n",
        "        The MLP model.\n",
        "    \"\"\"\n",
        "    width = input_shape\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(width, 100),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(100, 100),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(100, 2),\n",
        "        torch.nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPMYfBpqBXEQ"
      },
      "source": [
        "**Example Implementation:** compute the H-Divergence between two randomly chosen subsets of MNIST (should be close to zero)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oowbf9IxBXER"
      },
      "outputs": [],
      "source": [
        "# set up functions for MNIST example\n",
        "\n",
        "# load the MNIST dataset\n",
        "\n",
        "def load_mnist():\n",
        "    \"\"\"\n",
        "    A function that loads the MNIST dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_train : torch.Tensor\n",
        "        The training data.\n",
        "\n",
        "    y_train : torch.Tensor\n",
        "        The training labels.\n",
        "\n",
        "    X_test : torch.Tensor\n",
        "        The test data.\n",
        "\n",
        "    y_test : torch.Tensor\n",
        "        The test labels.\n",
        "    \"\"\"\n",
        "    # load the MNIST dataset from torchvision\n",
        "    train = torchvision.datasets.MNIST('./files/', train=True, download=True)\n",
        "    test = torchvision.datasets.MNIST('./files/', train=False, download=True)\n",
        "\n",
        "    # convert the data to numpy arrays\n",
        "    X_train = train.data.numpy()\n",
        "    y_train = train.targets.numpy()\n",
        "    X_test = test.data.numpy()\n",
        "    y_test = test.targets.numpy()\n",
        "\n",
        "    # reshape the data\n",
        "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # normalize the data\n",
        "    X_train = X_train / 255\n",
        "    X_test = X_test / 255\n",
        "\n",
        "    # convert the data to torch tensors\n",
        "    X_train = torch.from_numpy(X_train).float()\n",
        "    y_train = torch.from_numpy(y_train).long()\n",
        "    X_test = torch.from_numpy(X_test).float()\n",
        "    y_test = torch.from_numpy(y_test).long()\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "# randomly choose 500 samples from the training data\n",
        "\n",
        "def choose_initial_samples(X_train, y_train):\n",
        "    \"\"\"\n",
        "    A function that randomly chooses 500 samples from the training data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : torch.Tensor\n",
        "        The training data.\n",
        "\n",
        "    y_train : torch.Tensor\n",
        "        The training labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_labeled : torch.Tensor\n",
        "        The labeled data.\n",
        "\n",
        "    y_labeled : torch.Tensor\n",
        "        The labeled labels.\n",
        "    \"\"\"\n",
        "    # randomly choose 500 samples from the training data\n",
        "    idx = np.random.choice(X_train.shape[0], 500, replace=False)\n",
        "    X_labeled = X_train[idx]\n",
        "    y_labeled = y_train[idx]\n",
        "\n",
        "    return X_labeled, y_labeled\n",
        "\n",
        "\n",
        "# define a 5-layer CNN model in torch to train on MNIST\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"\n",
        "    The CNN model for semi-supervised active learning, without any regularization techniques.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : torch.nn.Sequential\n",
        "        The CNN model.\n",
        "    \"\"\"\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        torch.nn.Dropout(0.25),\n",
        "        torch.nn.Flatten(),\n",
        "        torch.nn.Linear(7 * 7 * 64, 128),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Dropout(0.5),\n",
        "        torch.nn.Linear(128, 10),\n",
        "        torch.nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train the model on the labeled data (note that we don't really care about the test accuracy here)\n",
        "\n",
        "def train_model(X_labeled, y_labeled):\n",
        "    \"\"\"\n",
        "    A function that trains and returns a CNN model on the labeled data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_labeled : torch.Tensor\n",
        "        The labeled data.\n",
        "\n",
        "    y_labeled : torch.Tensor\n",
        "        The labeled labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : tf.keras.Sequential\n",
        "        The trained CNN model.\n",
        "    \"\"\"\n",
        "    # build the model\n",
        "    model = get_model()\n",
        "\n",
        "    # train the model\n",
        "    batch_size = 100\n",
        "    epochs = 10\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, X_labeled.shape[0], batch_size):\n",
        "            x = X_labeled[i:i + batch_size]\n",
        "            y = y_labeled[i:i + batch_size]\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# get the latent representation of the data using the trained model\n",
        "\n",
        "def get_latent_representation(model, X):\n",
        "    \"\"\"\n",
        "    A function that computes the latent representation of the data using the trained model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : torch.nn.Sequential\n",
        "        The trained CNN model.\n",
        "\n",
        "    X : torch.Tensor\n",
        "        The data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    latent_representation : numpy.ndarray\n",
        "        The latent representation of the data.\n",
        "    \"\"\"\n",
        "    # get the latent representation of the data using the trained model\n",
        "    intermediate_layer_model = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "    latent_representation = intermediate_layer_model.predict(X)\n",
        "\n",
        "    return latent_representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DhACWh8iBXET",
        "outputId": "1403a01c-7017-46e6-9ed6-18bd832d6c6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5/5 - 11s - loss: 2.1448 - accuracy: 0.2740 - 11s/epoch - 2s/step\n",
            "Epoch 2/10\n",
            "5/5 - 0s - loss: 1.4613 - accuracy: 0.5720 - 44ms/epoch - 9ms/step\n",
            "Epoch 3/10\n",
            "5/5 - 0s - loss: 0.9827 - accuracy: 0.6980 - 43ms/epoch - 9ms/step\n",
            "Epoch 4/10\n",
            "5/5 - 0s - loss: 0.7355 - accuracy: 0.7580 - 43ms/epoch - 9ms/step\n",
            "Epoch 5/10\n",
            "5/5 - 0s - loss: 0.5339 - accuracy: 0.8500 - 46ms/epoch - 9ms/step\n",
            "Epoch 6/10\n",
            "5/5 - 0s - loss: 0.4962 - accuracy: 0.8440 - 42ms/epoch - 8ms/step\n",
            "Epoch 7/10\n",
            "5/5 - 0s - loss: 0.3872 - accuracy: 0.8760 - 38ms/epoch - 8ms/step\n",
            "Epoch 8/10\n",
            "5/5 - 0s - loss: 0.3678 - accuracy: 0.8760 - 37ms/epoch - 7ms/step\n",
            "Epoch 9/10\n",
            "5/5 - 0s - loss: 0.2959 - accuracy: 0.9200 - 37ms/epoch - 7ms/step\n",
            "Epoch 10/10\n",
            "5/5 - 0s - loss: 0.2117 - accuracy: 0.9380 - 35ms/epoch - 7ms/step\n"
          ]
        }
      ],
      "source": [
        "# get x_labeled and y_labeled from the previous step and train the model\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_mnist()\n",
        "X_labeled, y_labeled = choose_initial_samples(X_train, y_train)\n",
        "model = train_model(X_labeled, y_labeled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ponm7UjLBXEU",
        "outputId": "1596a2c6-f933-4508-db06-3ba52e36b672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 4ms/step\n",
            "1875/1875 [==============================] - 3s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "605/605 - 3s - loss: 0.0846 - accuracy: 0.9917 - 3s/epoch - 5ms/step\n",
            "Epoch 2/10\n",
            "605/605 - 1s - loss: 0.0483 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "605/605 - 2s - loss: 0.0481 - accuracy: 0.9917 - 2s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "605/605 - 1s - loss: 0.0481 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "605/605 - 1s - loss: 0.0481 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "605/605 - 1s - loss: 0.0479 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "605/605 - 1s - loss: 0.0480 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "605/605 - 1s - loss: 0.0481 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "605/605 - 1s - loss: 0.0480 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "605/605 - 2s - loss: 0.0480 - accuracy: 0.9917 - 2s/epoch - 3ms/step\n",
            "16/16 [==============================] - 0s 2ms/step\n",
            "1875/1875 [==============================] - 3s 2ms/step\n",
            "0.0005562470753987629\n"
          ]
        }
      ],
      "source": [
        "# compute H-Divergence between X_labeled and X_train\n",
        "\n",
        "# get the latent representation of the labeled data and the training data\n",
        "\n",
        "latent_representation_labeled = get_latent_representation(model, X_labeled)\n",
        "latent_representation_train = get_latent_representation(model, X_train)\n",
        "\n",
        "\n",
        "# define a discriminator model with input shape the size of the latent representation\n",
        "# and train it on the latent representation of the labeled data and the training data\n",
        "\n",
        "discriminator = train_discriminative_model(latent_representation_labeled, \n",
        "                                           latent_representation_train, latent_representation_labeled.shape[1])\n",
        "\n",
        "\n",
        "# compute the H-Divergence between the latent representation of the labeled data and the training data\n",
        "\n",
        "H_divergence = compute_H_divergence(latent_representation_labeled, latent_representation_train, discriminator)\n",
        "\n",
        "print(H_divergence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpHbs0tlGbJh"
      },
      "source": [
        "**Interpretation of Results:** The H-Divergence in this case is essentially zero, which is what we expected as the samples were chosen randomly."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5beae9835b3277c3a4a8c87413b972e297eaccb765a3f62b691c35696bfb6223"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
